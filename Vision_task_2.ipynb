{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nik1806/Semantic-segmentation/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zuklJ8hzEwLs",
    "outputId": "c6424043-8e7e-4da4-fc32-4f80451632e5"
   },
   "outputs": [],
   "source": [
    "# mounting google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JDOrGbbpFJZf"
   },
   "outputs": [],
   "source": [
    "# Load the data and organizing it \n",
    "# root_dir = 'drive/My Drive/'\n",
    "# data_dir = root_dir + 'NN_pro_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jPkjKXMJFVKc"
   },
   "outputs": [],
   "source": [
    "# change directory\n",
    "# import os\n",
    "# os.chdir(data_dir)\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OgVWoPKwGUjD"
   },
   "outputs": [],
   "source": [
    "# unzip data \n",
    "#!unzip gtFine_trainvaltest.zip\n",
    "# !unzip leftImg8bit_trainvaltest.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSuaMW1rHtRr",
    "outputId": "391599b2-aac9-4d4b-95a5-a5ce9d9aa7a3"
   },
   "outputs": [],
   "source": [
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QKQD0uCZKq49"
   },
   "outputs": [],
   "source": [
    "# Insert the directory - FOR importing user-defined modules/packages\n",
    "# import sys\n",
    "# sys.path.insert(0,'/content/drive/My Drive/Semantic-segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XJxgW61OEosh"
   },
   "outputs": [],
   "source": [
    "# from dataset import cityscapes_train\n",
    "# from dataset import cityscapes_val\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import Cityscapes\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets.custom_transforms import decode_segmap, unNormalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmfX_R7nEosv"
   },
   "source": [
    "### Dataset reader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tw7FK90KEosv"
   },
   "outputs": [],
   "source": [
    "#from utils.custom_transforms import pair_transform_train, pair_transform_val\n",
    "#data_dir = '../datasets/cityscapes/'\n",
    "#dst_train = Cityscapes(data_dir, split='train', mode='fine', target_type='semantic', transforms=pair_transform_train) # data_dir\n",
    "#dst_val = Cityscapes(data_dir, split='val', mode='fine', target_type='semantic', transforms=pair_transform_val)\n",
    "## data length\n",
    "#length_train = len(dst_train)\n",
    "#length_val = len(dst_val)\n",
    "#print(length_train)\n",
    "#print(length_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Bm5yK9XXhKpH"
   },
   "outputs": [],
   "source": [
    "from datasets import cityscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtUHjgtXEosw",
    "outputId": "cf808bc6-9708-4863-ee87-f816b6e5f20b"
   },
   "outputs": [],
   "source": [
    "# data directory path\n",
    "data_dir = '../datasets/cityscapes/'\n",
    "base_size = 288\n",
    "crop_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2975 train images\n",
      "Found 500 val images\n"
     ]
    }
   ],
   "source": [
    "# dataset readers\n",
    "dst_train = cityscapes.CityscapesSegmentation(crop_size, base_size, root=data_dir, split='train')\n",
    "dst_val = cityscapes.CityscapesSegmentation(crop_size, base_size, root=data_dir, split='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p7nQlQAWNxeV"
   },
   "outputs": [],
   "source": [
    "#from utils.custom_transforms import unNormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "V62GG61ENbfb",
    "outputId": "f76ef3fe-91f4-4b11-8a36-fd49b4ebf64e"
   },
   "outputs": [],
   "source": [
    "## Verifying transformations (Especially consistancy among image and segmentation map)\n",
    "\n",
    "# # img, segmap = dst_train[16]\n",
    "# img, segmap = dst_val[21]\n",
    "\n",
    "# fig = plt.figure(figsize=(10, 7))\n",
    "\n",
    "# fig.add_subplot(1,2,1)\n",
    "# img = img.data.numpy() # data in image and current form of matrix\n",
    "# img = unNormalize(img, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # unNormalize\n",
    "# img = img.transpose((1,2,0)).astype(np.uint8) # change dtype to correct format for display\n",
    "# plt.title('Original')\n",
    "# plt.imshow(img) # original\n",
    "# plt.axis('off')\n",
    "\n",
    "# # ground truth\n",
    "# fig.add_subplot(1, 2, 2)\n",
    "# label = segmap.data.numpy() # data in image and current form of matrix\n",
    "# # label = decode_segmap(label)\n",
    "# plt.title('Ground truth')\n",
    "# plt.imshow(label) \n",
    "# plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WwHWIGqfEosy"
   },
   "outputs": [],
   "source": [
    "bs = 10\n",
    "cls_num = 19\n",
    "#Dataloaders\n",
    "train_loader = DataLoader(dst_train, batch_size=bs, shuffle=True)\n",
    "val_loader = DataLoader(dst_val, batch_size=bs,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CUGPTBncEosy"
   },
   "outputs": [],
   "source": [
    "# dataiter = iter(train_loader)\n",
    "# #print(dataiter.next())\n",
    "# (images, labels) = dataiter.next()\n",
    "\n",
    "# dataiter_val = iter(val_loader)\n",
    "# (images_val, labels_val) = dataiter_val.next()\n",
    "# labels_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aVmKFCxjEosy"
   },
   "outputs": [],
   "source": [
    "from models.networkT2 import R2UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-CTFTCLoEosz"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:6\")\n",
    "model = R2UNet(in_channels=3,n_classes=cls_num).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "lr = 3e-4\n",
    "wdk = 5e-4 #earlier 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-vj_jdjgEosz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam, lr_scheduler\n",
    "criterion = torch.nn.CrossEntropyLoss(size_average=True, ignore_index=255)\n",
    "optimizer = Adam(model.parameters(), lr, (0.9, 0.999),  eps=1e-08, weight_decay=wdk) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X9tMY3mBEosz",
    "outputId": "15a6c38f-e50d-44d1-efe7-e24cfd624505",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0, iter0, running loss: 0.5798403739929199\n",
      "Epoch0, iter60, running loss: 0.19672707182462099\n",
      "Epoch0, iter120, running loss: 0.15734175793395555\n",
      "Epoch0, iter180, running loss: 0.1401642770727695\n",
      "Epoch0, iter240, running loss: 0.13154896371592129\n",
      "Epoch0, Training loss: 1.2511200074781508\n",
      "Epoch0, Validation loss: 1.7938858342170716\n",
      "Epoch1, iter0, running loss: 0.10356841087341309\n",
      "Epoch1, iter60, running loss: 0.09139333680027821\n",
      "Epoch1, iter120, running loss: 0.08974680565605479\n",
      "Epoch1, iter180, running loss: 0.08900963160214503\n",
      "Epoch1, iter240, running loss: 0.08731157139129164\n",
      "Epoch1, Training loss: 0.8713546127280933\n",
      "Epoch1, Validation loss: 0.7993522143363953\n",
      "Epoch2, iter0, running loss: 0.07310233116149903\n",
      "Epoch2, iter60, running loss: 0.07915941994698321\n",
      "Epoch2, iter120, running loss: 0.08045784823658053\n",
      "Epoch2, iter180, running loss: 0.07934139545451212\n",
      "Epoch2, iter240, running loss: 0.07924820778280868\n",
      "Epoch2, Training loss: 0.7800984491637889\n",
      "Epoch2, Validation loss: 0.976611135005951\n",
      "Epoch3, iter0, running loss: 0.07725489139556885\n",
      "Epoch3, iter60, running loss: 0.07268255279689538\n",
      "Epoch3, iter120, running loss: 0.07435471713542938\n",
      "Epoch3, iter180, running loss: 0.07259416056601382\n",
      "Epoch3, iter240, running loss: 0.0721708728070081\n",
      "Epoch3, Training loss: 0.7251703280130489\n",
      "Epoch3, Validation loss: 0.8988247179985046\n",
      "Epoch4, iter0, running loss: 0.07338652014732361\n",
      "Epoch4, iter60, running loss: 0.07221445240935341\n",
      "Epoch4, iter120, running loss: 0.07168486837513191\n",
      "Epoch4, iter180, running loss: 0.07034952358975595\n",
      "Epoch4, iter240, running loss: 0.07005776902699372\n",
      "Epoch4, Training loss: 0.6930391615469184\n",
      "Epoch4, Validation loss: 0.5600848257541656\n",
      "Epoch5, iter0, running loss: 0.0748814046382904\n",
      "Epoch5, iter60, running loss: 0.06915282289512822\n",
      "Epoch5, iter120, running loss: 0.06769017926917588\n",
      "Epoch5, iter180, running loss: 0.06670738285747023\n",
      "Epoch5, iter240, running loss: 0.06655747205389981\n",
      "Epoch5, Training loss: 0.6606514577897603\n",
      "Epoch5, Validation loss: 0.49615401685237887\n",
      "Epoch6, iter0, running loss: 0.044749277830123904\n",
      "Epoch6, iter60, running loss: 0.06493223656396396\n",
      "Epoch6, iter120, running loss: 0.06377496007552817\n"
     ]
    }
   ],
   "source": [
    "loss_train = []\n",
    "loss_val = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #Training\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    #for i, (images, labels) in enumerate(train_loader):\n",
    "    for i, samples in enumerate(train_loader):\n",
    "\n",
    "        #inputs = images.to(device)\n",
    "        #labels = labels.to(device)\n",
    "        inputs = samples['image'].to(device)\n",
    "        labels = samples['label'].to(device).long()\n",
    "        ## labels = labels.squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "     \n",
    "    #loss = criterion(outputs,targets[:,0])\n",
    "        #loss = criterion(outputs,labels[:,0].long())\n",
    "        # loss = criterion(outputs,labels.long())\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ###accumulating loss for each batch\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i%60 == 0:\n",
    "            print(\"Epoch{}, iter{}, running loss: {}\".format(epoch, i, running_loss/(bs*(i+1))))\n",
    "                  \n",
    "    loss_train.append(running_loss/len(train_loader))\n",
    "\n",
    "    print(\"Epoch{}, Training loss: {}\".format(epoch, running_loss/len(train_loader)))\n",
    "    torch.save(model.state_dict(), f'../weights/T2/epoch_{epoch}.pth')\n",
    "\n",
    "    #Validation\n",
    "    model.eval()\n",
    "    running_loss_val = 0\n",
    "    for i, samples in enumerate(val_loader):\n",
    "        inputs = samples['image'].to(device)\n",
    "        labels = samples['label'].to(device).long()\n",
    "        # labels = labels.squeeze(1)\n",
    "             \n",
    "        with torch.no_grad(): \n",
    "            outputs = model(inputs)\n",
    "            # loss = criterion(outputs,labels.long())\n",
    "            loss = criterion(outputs,labels)\n",
    "\n",
    "            ###accumulating loss for each batch\n",
    "            running_loss_val += loss.item()\n",
    "\n",
    "\n",
    "        #if i%10 == 0:\n",
    "    loss_val.append(running_loss_val/len(val_loader))\n",
    "    print(\"Epoch{}, Validation loss: {}\".format(epoch, running_loss_val/len(val_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O26vshFlEos0",
    "outputId": "409ca9b6-012c-4735-d682-f5e74a7afee6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# x = [i for i in range(len(train_loader))]\n",
    "x = range(1, epochs+1)\n",
    "plt.title(\"Plot showing training and validation loss against number of epochs\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(x, loss_train, color='b', label='Training loss')\n",
    "plt.plot(x, loss_val, color='r', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.savefig('results/T2_loss_curves.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "# plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# epochs=1\n",
    "# move model to gpu\n",
    "model = model.to(device)\n",
    "# load the model states\n",
    "model.load_state_dict(torch.load(f'../weights/T2/epoch_{epochs-1}.pth'))\n",
    "\n",
    "# model in evaluation model -> batchnorm, dropout etc. adjusted accordingly\n",
    "model.eval()\n",
    "# iterator on training data\n",
    "data = iter(train_loader)\n",
    "# init figure object\n",
    "fig = plt.figure(figsize=(10,40))\n",
    "pred_rgb = list()\n",
    "# for img, label in trainloader:\n",
    "for i in range(10):\n",
    "    sample = next(data) # next batch\n",
    "    imgs, labels = sample['image'], sample['label']\n",
    "    # img, label = img.to(device).unsqueeze(0), label.to(device) # to gpu\n",
    "    # using just one image\n",
    "    img = imgs[0].to(device).unsqueeze(0) # to gpu & add dummy batch dim\n",
    "    # gnd = np.asarray(label[0]) \n",
    "    # deactivate autograd engine - reduce memory usage \n",
    "    with torch.no_grad(): \n",
    "        pred = model(img) # forward pass\n",
    "        # output of model is orderedDict\n",
    "#         pred = pred['out'] # 21(class)xHxW\n",
    "        pred = pred.squeeze(0)\n",
    "        \n",
    "        # extract most probable class through C-dim \n",
    "        pred_label = torch.argmax(pred, dim=0).cpu().numpy()\n",
    "        # convert labels to color code\n",
    "#         pred_rgb = dst_train.decode_segmap(pred_label)\n",
    "        pred_rgb = decode_segmap(pred_label, nc=cls_num, dataset='cityscapes')\n",
    "\n",
    "        # plotting\n",
    "        # original image\n",
    "        fig.add_subplot(10, 3, 3*i+1)\n",
    "        img = imgs[0].data.cpu().numpy() # data in image and current form of matrix\n",
    "        img = unNormalize(img, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # unNormalize\n",
    "        img = img.transpose((1,2,0)).astype(np.uint8) # change dtype to correct format for display\n",
    "        plt.title('Original')\n",
    "        plt.imshow(img) # original\n",
    "        plt.axis('off')\n",
    "        # ground truth\n",
    "        fig.add_subplot(10, 3, 3*i+2)\n",
    "        label = labels[0].data.numpy() # data in image and current form of matrix\n",
    "        label = decode_segmap(label, nc=cls_num, dataset='cityscapes')\n",
    "#         label = dst_train.decode_segmap(label)\n",
    "        plt.title('Ground truth')\n",
    "        plt.imshow(label) \n",
    "        plt.axis('off')\n",
    "        # prediction\n",
    "        fig.add_subplot(10, 3, 3*i+3)\n",
    "        plt.title('Prediction')\n",
    "        plt.imshow(pred_rgb.astype(np.uint8))\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.savefig('results/T2 results.png', bbox_inches='tight')    \n",
    "plt.plot()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and plotting \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJJsiFQhEos1"
   },
   "outputs": [],
   "source": [
    "#5 evaluation matrices to be used: sensitivity, specificity, accuracy, AUC, and DC\n",
    "from utils.eval_metrics import dice_coefficient_custom, roc_auc_custom, accuracy_se_sp_custom, sensitivity_custom, specificity_custom# user defined\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, multilabel_confusion_matrix\n",
    "\n",
    "def evaluate_batch(gnd_b, pred_b, cls:int = 34):\n",
    "    \"\"\"\n",
    "        Calculate evalution scores over the batch.\n",
    "    Args:   \n",
    "        gnd_b: BxHxW tensor; ground truth labels; each element of matrix in B dim contains class label from (0-20)\n",
    "        pred_b: BxCxHxW tensor; each element contains predicted class label \n",
    "                here C=21 (0-20; no. of classes); each C corresponds to probabilites for that class,\n",
    "                eg. C=0 contain score at each element in matrix HxW \n",
    "    Return:\n",
    "        f1_score, auc_score, dice_coeeficient (averaged over batch size)\n",
    "    \"\"\"\n",
    "    # to cpu and as numpy ndarray\n",
    "    gnd_b = gnd_b.cpu()\n",
    "\n",
    "    batch_size = gnd_b.shape[0]\n",
    "    \n",
    "    # extract most probable class through C-dim \n",
    "    label_b = torch.argmax(pred_b, dim=1).cpu()\n",
    "    sensitivity = specificity = accuracy = auc = dice = 0\n",
    "    # iterate over batch elements\n",
    "    for i in range(batch_size):\n",
    "        gnd = gnd_b[i,:,:] \n",
    "        label = label_b[i,:,:]\n",
    "        #f1 += f1_score(gnd.flatten(), label.flatten(), average='micro')\n",
    "        #sensitivity += sensitivity_custom(gnd, label)\n",
    "        \n",
    "        \n",
    "        #specificity += specificity_custom(gnd, label)\n",
    "        #accuracy += accuracy_se_sp_custom(gnd, label)\n",
    "        temp = accuracy_se_sp_custom(gnd, label)\n",
    "        accuracy += temp[0]\n",
    "        sensitivity += temp[1]\n",
    "        specificity += temp[2]\n",
    "        # auc += roc_auc_score(gnd.flatten(), label.flatten(), average='micro', multi_class='ovr')\n",
    "        auc += roc_auc_custom(gnd.numpy(), label.numpy(), cls, average='micro')\n",
    "        dice += dice_coefficient_custom(gnd.numpy(), label.numpy(), cls)\n",
    "\n",
    "    return [sensitivity/batch_size, specificity/batch_size, accuracy/batch_size, auc/batch_size, dice/batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of score \n",
    "sensitivity_list = []\n",
    "specificity_list = []\n",
    "accuracy_list = []\n",
    "auc_list = []\n",
    "dice_list = []\n",
    "\n",
    "# For print\n",
    "print('Epochs\\t Sensitivity-score Specificity-score Accuracy-score ROC-AUC-score\\t Dice score')\n",
    "\n",
    "# move model to gpu\n",
    "model = model.to(device)\n",
    "# loop for original number of epochs\n",
    "for i in range(epochs):\n",
    "    # load the model states\n",
    "    model.load_state_dict(torch.load(f'../weights/T2/epoch_{i}.pth'))\n",
    "    # model in evaluation model -> batchnorm, dropout etc. adjusted accordingly\n",
    "    model.eval()\n",
    "    # evaluation score variables to store values over each epoch\n",
    "    sensitivity_score = specificity_score = accuracy_score = auc_score = dice_score = 0\n",
    "\n",
    "\n",
    "    for img, label in val_loader:\n",
    "        img, label = img.to(device), label.to(device) # to gpu\n",
    "        # deactivate autograd engine - reduce memory usage \n",
    "        with torch.no_grad(): \n",
    "            pred = model(img) # forward pass\n",
    "            # output of model is orderedDict\n",
    "#             pred = pred['out'] # Batchx21(class)xHxW\n",
    "            # evaluation\n",
    "            scores = evaluate_batch(label, pred, cls=cls_num)\n",
    "            # sum values\n",
    "            sensitivity_score += scores[0]\n",
    "            specificity_score += scores[1]\n",
    "            accuracy_score += scores[2]\n",
    "            auc_score += scores[3]\n",
    "            dice_score += scores[4]\n",
    "    \n",
    "        # break\n",
    "\n",
    "    print('{}\\t {:.3f}\\t\\t\\t {:.3f}\\t\\t {:.3f}\\t\\t {:.3f}\\t\\t {:.3f}'.format(i ,sensitivity_score/len(val_loader), specificity_score/len(val_loader), accuracy_score/len(val_loader), auc_score/len(val_loader), dice_score/len(val_loader)))\n",
    "    # append to list (with averaged values over valid set)\n",
    "    sensitivity_list.append(sensitivity_score/len(val_loader))\n",
    "    specificity_list.append(specificity_score/len(val_loader))\n",
    "    accuracy_list.append(accuracy_score/len(val_loader))\n",
    "    auc_list.append(auc_score/len(val_loader))\n",
    "    dice_list.append(dice_score/len(val_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT\n",
    "X = range(1, epochs+1)\n",
    "plt.plot(X, sensitivity_list, label=\"sensitivity-score\")\n",
    "plt.plot(X, specificity_list, label=\"specificity-score\")\n",
    "plt.plot(X, accuracy_list, label=\"accuracy-score\")\n",
    "plt.plot(X, auc_list, label=\"AUC-ROC score\")\n",
    "plt.plot(X, dice_list, label=\"Dice coefficient\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Evaluation metrics score\")\n",
    "plt.title(\"Performance evalaution\")\n",
    "plt.legend() # add legend\n",
    "plt.savefig('results/T2_eval_metrics.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copy of main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
