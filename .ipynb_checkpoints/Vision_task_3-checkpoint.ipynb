{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short description about attempt/idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# from datasets.custom_transforms import unNormalize, decode_segmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dataset reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import cityscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory path\n",
    "data_dir = '../datasets/cityscapes/'\n",
    "# for augmentation\n",
    "base_size = 288 # original image resize to 'base_size' dim\n",
    "crop_size = 256 # finally a random crop is performed and image of 'crop_size' dim is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2975 train images\n"
     ]
    }
   ],
   "source": [
    "# dataset readers\n",
    "# additional augmentations are defined inside the reader\n",
    "dst_train = cityscapes.CityscapesSegmentation(crop_size, base_size, root=data_dir, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 val images\n"
     ]
    }
   ],
   "source": [
    "dst_val = cityscapes.CityscapesSegmentation(crop_size, base_size, root=data_dir, split='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 42 # maximum possible with image resolution, model size and available\n",
    "cls_num = 19 # ignoring rare classes which cause class imbalance problem\n",
    "#Dataloaders\n",
    "train_loader = DataLoader(dst_train, batch_size=bs, shuffle=True)\n",
    "val_loader = DataLoader(dst_val, batch_size=bs,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "from models.networkT3 import DeepLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define which gpu to use\n",
    "device = torch.device(\"cuda:5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/zhanghang1989/ResNeSt/archive/master.zip\" to /home/nipa00002/.cache/torch/hub/master.zip\n",
      "Using cache found in /home/nipa00002/.cache/torch/hub/zhanghang1989_ResNeSt_master\n"
     ]
    }
   ],
   "source": [
    "# create instance of model\n",
    "model = DeepLab(num_classes=cls_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 70\n",
    "lr = 3e-4\n",
    "mlr = 1e-2\n",
    "wdk = 4e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam, lr_scheduler\n",
    "# loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(size_average=True, ignore_index=255) # 255 label assigned to ignored classes\n",
    "# optimizer\n",
    "optimizer = Adam(model.parameters(), mlr, (0.9, 0.999),  eps=1e-08, weight_decay=wdk)\n",
    "# learning rate scheduler (update after every batch)\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=mlr, steps_per_epoch=len(train_loader), epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter0, running loss: 0.0875198103132702\n",
      "epoch0, iter60, running loss: 0.025418779516852896\n",
      "epoch0, Training loss: 1.0104147103470815\n",
      "epoch0, Validation loss: 0.5039363900820414\n",
      "epoch1, iter0, running loss: 0.015480277084168933\n",
      "epoch1, iter60, running loss: 0.013011586831287888\n",
      "epoch1, Training loss: 0.5381001477510157\n",
      "epoch1, Validation loss: 0.4015345300237338\n",
      "epoch2, iter0, running loss: 0.012902746597925821\n",
      "epoch2, iter60, running loss: 0.011956190627873828\n",
      "epoch2, Training loss: 0.5027136202429382\n",
      "epoch2, Validation loss: 0.4096009110411008\n",
      "epoch3, iter0, running loss: 0.008635501776422774\n",
      "epoch3, iter60, running loss: 0.010848553035521675\n",
      "epoch3, Training loss: 0.45839822670103797\n",
      "epoch3, Validation loss: 0.3794664020339648\n",
      "epoch4, iter0, running loss: 0.010832515500840686\n",
      "epoch4, iter60, running loss: 0.01031121504576666\n",
      "epoch4, Training loss: 0.42810890036569516\n",
      "epoch4, Validation loss: 0.3664625957608223\n",
      "epoch5, iter0, running loss: 0.010929280803317116\n",
      "epoch5, iter60, running loss: 0.011083420764460032\n",
      "epoch5, Training loss: 0.4670491785230771\n",
      "epoch5, Validation loss: 0.48319947471221286\n",
      "epoch6, iter0, running loss: 0.013441531431107294\n",
      "epoch6, iter60, running loss: 0.010990789586170682\n",
      "epoch6, Training loss: 0.464931157693057\n",
      "epoch6, Validation loss: 0.5353731835881869\n",
      "epoch7, iter0, running loss: 0.011101992357344855\n",
      "epoch7, iter60, running loss: 0.01178009748738041\n",
      "epoch7, Training loss: 0.4936706935855704\n",
      "epoch7, Validation loss: 0.43027909596761066\n",
      "epoch8, iter0, running loss: 0.011123382619449071\n",
      "epoch8, iter60, running loss: 0.011226469236458772\n",
      "epoch8, Training loss: 0.46827033875693735\n",
      "epoch8, Validation loss: 0.4187810942530632\n",
      "epoch9, iter0, running loss: 0.010331989753813971\n",
      "epoch9, iter60, running loss: 0.011892056448863503\n",
      "epoch9, Training loss: 0.5000145792121619\n",
      "epoch9, Validation loss: 0.5440071622530619\n",
      "epoch10, iter0, running loss: 0.011254653334617615\n",
      "epoch10, iter60, running loss: 0.011682136230036954\n",
      "epoch10, Training loss: 0.4871956941107629\n",
      "epoch10, Validation loss: 0.5263969153165817\n",
      "epoch11, iter0, running loss: 0.012879746300833566\n",
      "epoch11, iter60, running loss: 0.011919069718235085\n",
      "epoch11, Training loss: 0.5030373897351009\n",
      "epoch11, Validation loss: 0.41302941491206485\n",
      "epoch12, iter0, running loss: 0.011412568035579863\n",
      "epoch12, iter60, running loss: 0.010767869005716935\n",
      "epoch12, Training loss: 0.4564235050913314\n",
      "epoch12, Validation loss: 0.49383407086133957\n",
      "epoch13, iter0, running loss: 0.01103761721225012\n",
      "epoch13, iter60, running loss: 0.010796426703090504\n",
      "epoch13, Training loss: 0.46070301112994344\n",
      "epoch13, Validation loss: 0.8390689392884573\n",
      "epoch14, iter0, running loss: 0.010176182502791994\n",
      "epoch14, iter60, running loss: 0.011865046413311448\n",
      "epoch14, Training loss: 0.5032580381547901\n",
      "epoch14, Validation loss: 1.1585393448670704\n",
      "epoch15, iter0, running loss: 0.0116839295341855\n",
      "epoch15, iter60, running loss: 0.012071767407502168\n",
      "epoch15, Training loss: 0.5020742235888898\n",
      "epoch15, Validation loss: 0.45591021080811817\n",
      "epoch16, iter0, running loss: 0.0099003818773088\n",
      "epoch16, iter60, running loss: 0.011153730584903213\n",
      "epoch16, Training loss: 0.4642173299487208\n",
      "epoch16, Validation loss: 0.8044809599717458\n",
      "epoch17, iter0, running loss: 0.011022923248154777\n",
      "epoch17, iter60, running loss: 0.011454120999570008\n",
      "epoch17, Training loss: 0.47575073384902844\n",
      "epoch17, Validation loss: 0.4195927456021309\n",
      "epoch18, iter0, running loss: 0.010241793025107611\n",
      "epoch18, iter60, running loss: 0.010741858895079965\n",
      "epoch18, Training loss: 0.4516952230896748\n"
     ]
    }
   ],
   "source": [
    "from utils.learn import train_val_loop\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "\n",
    "# perform loops\n",
    "loss_train, loss_val = train_val_loop(model, epochs, train_loader, val_loader, optimizer, \n",
    "                                      criterion, loss_train, 3, bs, device, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss curves\n",
    "x = range(epochs)\n",
    "plt.title(\"Plot showing training and validation loss against number of epochs\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(x, loss_train, color='b', label='Training loss')\n",
    "plt.plot(x, loss_val, color='r', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.savefig('results/T3_loss_curves.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visual_results import visualize\n",
    "# epochs=1\n",
    "# move model to gpu\n",
    "model = model.to(device)\n",
    "# load the model states\n",
    "model.load_state_dict(torch.load(f'../weights/T3/epoch_{epochs-1}.pth'))\n",
    "# perform visualization\n",
    "visualize(model, train_loader, 'results/T3 results.png', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval_metrics import evaluation_loop\n",
    "\n",
    "# #6 evaluation matrices to be used: sensitivity, specificity, accuracy, AUC, DC and IOU\n",
    "evaluation_loop(model, val_loader, epochs, device, task=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT\n",
    "df_r = pd.read_csv('results/T3_eval.csv')\n",
    "\n",
    "X = range(1, len(df_r)+1)\n",
    "plt.plot(X, df_r.sensitivity, label=\"sensitivity-score\")\n",
    "plt.plot(X, df_r.specificity, label=\"specificity-score\")\n",
    "plt.plot(X, df_r.accuracy, label=\"accuracy-score\")\n",
    "plt.plot(X, df_r.auc, label=\"AUC-ROC score\")\n",
    "plt.plot(X, df_r.dice, label=\"Dice coefficient\")\n",
    "plt.plot(X, df_r.iou, label=\"IOU-Jaccard\")\n",
    "\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Evaluation metrics score\")\n",
    "plt.title(\"Performance evalaution\")\n",
    "plt.legend() # add legend\n",
    "plt.savefig('results/T3_eval_metrics.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
