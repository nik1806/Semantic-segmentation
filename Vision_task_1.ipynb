{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nik1806/Semantic-segmentation/blob/master/Vision_task_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqryU0Zkgr2L"
   },
   "source": [
    "# Image Segmentation Task 1\n",
    "#### Welcome to the first task of Image Segmentation. Image segmentation is the process of partitioning the image into a set of pixels representing an object. In this task, you will be introduced to the problem of image segmentation and programming pipeline involved in image segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuWYYCszgr2j"
   },
   "source": [
    "For the purpose of this task we will be using PASCAL VOC datset. The dataset contains a total of 2913 images with segmentation annotations. Code in the cell below will download the code and extract the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zM_t4c-S3k31",
    "outputId": "993a5f16-03f5-4db5-f058-c9421e38d022",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
    "# !tar -xvf VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6lvs9XIpBaI0",
    "outputId": "046f45e4-c235-4255-8694-978b4c8b08d5"
   },
   "outputs": [],
   "source": [
    "# !pip install scipy==1.1.0\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggF1hZ6Vgr2n"
   },
   "source": [
    "### 1.1 Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qunDv45j24Mg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join as pjoin\n",
    "import collections\n",
    "import json\n",
    "import torch\n",
    "import imageio\n",
    "import numpy as np\n",
    "import scipy.misc as m\n",
    "import scipy.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# from PIL import Image\n",
    "import PIL.Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "from datasets.pascalVOC import pascalVOCDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O34skJ8pgr2u"
   },
   "source": [
    "### 1.2 Define the model architecture(2.0 point)\n",
    "In this section you have the freedom to decide your own model. Keep in mind though, to perform image segmentation, you need to implement an architecture that does pixel level classification i.e. for each pixel in the image you need to predict the probability of it belonging to one of the 21 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CatAsvH3GTXs"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# from torchvision.models import segmentation\n",
    "from models.networkT1 import UNet    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157,
     "referenced_widgets": [
      "6dc48abab91f41e38a03efc97fc88f5a",
      "b69f7c47a904498986d106e1bf36d926",
      "b4e581c1ceb9408790f30b96cff73f99",
      "ba09c1085fa74be4a05eced774156f7a",
      "b5680358c1e74addbddab3e381c36019",
      "58c5855440754107a3706f941d83e0f4",
      "e4d9fd672c704905a9a13c7d930dbad8",
      "d506f02006a648639ab6619118be8db1",
      "e91dacd227004bfbb066cd835e22afd3",
      "8560f77e7d414d4ba09d847a451df0e0",
      "0f8ff8022dae4990b1ea432b2e9d3c07",
      "92a04383007146e9b74ad6b15bc80413",
      "1d3610da2df64773b16b18f02fb4501a",
      "f0858d6174ed4af7b49a103e61536b04",
      "13ea534c6ce64663ad8e4202a002f51d",
      "bf9892dfa56c4cd6ae37fb8318044378"
     ]
    },
    "id": "QfQiOnEkGZat",
    "outputId": "547d08dc-de8c-4b40-ea30-5f362983f31f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /home/nipa00002/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8187852ae0b7402bb79d61360d08a0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating an instance of the model defined above. \n",
    "# You can modify it incase you need to pass paratemers to the constructor.\n",
    "# model = Segnet()\n",
    "model = UNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A6-PRU9gr2y"
   },
   "source": [
    "### 1.3 Hyperparameters(0.5 points)\n",
    "Define all the hyperparameters(not restricted to the three given below) that you find useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BgeL0Al7gr2y"
   },
   "outputs": [],
   "source": [
    "local_path = '../datasets/VOCdevkit/VOC2012/' # modify it according to your device\n",
    "bs = 32 # test with 16, 32 and 64 (select the best)\n",
    "epochs = 50 #5 # till 50 (5, 10, ...) (base on time also)\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXVHz5f7gr2z"
   },
   "source": [
    "### 1.4 Dataset and Dataloader(0.5 points)\n",
    "Create the dataset using pascalVOCDataset class defined above. Use local_path defined in the cell above as root. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_kJoNiXgr2z",
    "outputId": "4319942b-d3bf-431e-83e2-87523811bcc7"
   },
   "outputs": [],
   "source": [
    "# dataset variable\n",
    "dst_train = pascalVOCDataset(root=local_path, is_transform=True, img_size=256, split='train') # ADD AUGMENTATION\n",
    "dst_valid = pascalVOCDataset(root=local_path, is_transform=True, img_size=256, split='val')\n",
    "\n",
    "# dataloader variable\n",
    "# using in-built dataloader with reshuffling data at each epoch\n",
    "trainloader = data.DataLoader(dst_train, batch_size=bs, shuffle=True)\n",
    "validloader = data.DataLoader(dst_valid, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display pre-encoding\n",
    "from IPython.display import Image\n",
    "Image(filename='../datasets/VOCdevkit/VOC2012/SegmentationClass/pre_encoded/2007_000733.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnRC5XrLgr2z"
   },
   "source": [
    "### 1.5 Loss fuction and Optimizer(1.0 point)\n",
    "Define below with the loss function you think would be most suitable for segmentation task. You are free to choose any optimizer to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QgxxPYOsgr20"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# loss function\n",
    "# the problem is a classification problem (into 21 classes) CrossEntropyLoss works well in this case \n",
    "loss_f = nn.CrossEntropyLoss() \n",
    "\n",
    "# optimizer variable\n",
    "# Adam contains both momentum and rmsprop feature, it works well in general\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIbBCUnvgr20"
   },
   "source": [
    "### 1.6 Training the model(3.0 points)\n",
    "Your task here is to complete the code below to perform a training loop and save the model weights after each epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for gpu\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:4') # chose available gpu\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "\n",
    "# move model\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xz08hSdPKODm",
    "outputId": "c4e35c4f-1d25-4778-963c-1a9978d4e324",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0, iter0, running loss: 0.10271794348955154\n",
      "Epoch 0: Training loss=2.2600026519402214\n",
      "Epoch 0: Validation loss=2.6972916592722354\n",
      "epoch1, iter0, running loss: 0.053012024611234665\n",
      "Epoch 1: Training loss=1.5718825614970664\n",
      "Epoch 1: Validation loss=1.518447767133298\n",
      "epoch2, iter0, running loss: 0.0448482483625412\n",
      "Epoch 2: Training loss=1.3065371202385945\n",
      "Epoch 2: Validation loss=1.2490114180938057\n",
      "epoch3, iter0, running loss: 0.03717486560344696\n",
      "Epoch 3: Training loss=1.1370052915552389\n",
      "Epoch 3: Validation loss=1.1459603426249132\n",
      "epoch4, iter0, running loss: 0.032884567975997925\n",
      "Epoch 4: Training loss=1.0564085400622825\n",
      "Epoch 4: Validation loss=1.0967801822268444\n",
      "epoch5, iter0, running loss: 0.02894548699259758\n"
     ]
    }
   ],
   "source": [
    "# save losses in lists\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    # putting model in training mode\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for j, (img, label) in enumerate(trainloader):\n",
    "        # transfer to gpu if available for faster computation\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        # clean previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass through model\n",
    "        pred = model(img)\n",
    "#         pred = pred['out'] # output of model is orderedDict\n",
    "        # loss\n",
    "        loss = loss_f(pred, label)\n",
    "        total_loss += loss.item()\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # gradient descent -> update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        if j%60 == 0:\n",
    "            print(\"epoch{}, iter{}, running loss: {}\".format(e, j, total_loss/(bs*(j+1))))\n",
    "\n",
    "    # print training loss -> later print after epoch or half epoch (costly operation)\n",
    "    print(f'Epoch {e}: Training loss={total_loss/len(trainloader)}')\n",
    "    loss_train.append(total_loss/len(trainloader))\n",
    "\n",
    "    # saving model weights\n",
    "    torch.save(model.state_dict(), f'../weights/T1/epoch_{e}.pth')\n",
    "\n",
    "    # validation mode - batchnorm and dropout will working val mode\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    for (img, label) in validloader:\n",
    "        img, label = img.to(device), label.to(device) # to gpu\n",
    "        # deactivate autograd engine - reduce memory usage \n",
    "        with torch.no_grad(): \n",
    "            pred = model(img) # forward pass\n",
    "#             pred = pred['out'] # output of model is orderedDict\n",
    "            loss = loss_f(pred, label) # loss calc\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {e}: Validation loss={valid_loss/len(validloader)}')\n",
    "    loss_val.append(valid_loss/len(validloader))\n",
    "\n",
    "\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss curves\n",
    "X = range(1, epochs+1)\n",
    "plt.plot(X, loss_train, label=\"Training loss\")\n",
    "plt.plot(X, loss_val, label=\"Validation loss\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss curves\")\n",
    "plt.legend() # add legend\n",
    "plt.savefig('results/T1_loss_curves.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghWL_sQ6gr21"
   },
   "source": [
    "### 1.7 Evaluate your model(1.5 points)\n",
    "In this section you have to implement the evaluation metrics for your model. Calculate the values of F1-score, dice coefficient and AUC-ROC score on the data you used for training. You can use external packages like scikit-learn to compute above metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqYnTMNbgr21"
   },
   "outputs": [],
   "source": [
    "# import functions\n",
    "from sklearn.metrics import f1_score\n",
    "from utils.eval_metrics import dice_coefficient_custom, roc_auc_custom # user defined\n",
    "\n",
    "def evaluate_batch(gnd_b, pred_b):\n",
    "    \"\"\"\n",
    "        Calculate evalution scores over the batch.\n",
    "    Args:   \n",
    "        gnd_b: BxHxW tensor; ground truth labels; each element of matrix in B dim contains class label from (0-20)\n",
    "        pred_b: BxCxHxW tensor; each element contains predicted class label \n",
    "                here C=21 (0-20; no. of classes); each C corresponds to probabilites for that class,\n",
    "                eg. C=0 contain score at each element in matrix HxW \n",
    "    Return:\n",
    "        f1_score, auc_score, dice_coeeficient (averaged over batch size)\n",
    "    \"\"\"\n",
    "    # to cpu and as numpy ndarray\n",
    "    gnd_b = gnd_b.cpu().numpy()\n",
    "\n",
    "    batch_size = gnd_b.shape[0]\n",
    "    \n",
    "    # extract most probable class through C-dim \n",
    "    label_b = torch.argmax(pred_b, dim=1).cpu().numpy()\n",
    "\n",
    "    # initial value\n",
    "    f1 = auc = dice = 0\n",
    "    # iterate over batch elements\n",
    "    for i in range(batch_size):\n",
    "        gnd = gnd_b[i,:,:] \n",
    "        label = label_b[i,:,:]\n",
    "        f1 += f1_score(gnd.flatten(), label.flatten(), average='macro')\n",
    "        # auc += roc_auc_score(gnd.flatten(), label.flatten(), average='micro', multi_class='ovr')\n",
    "        auc += roc_auc_custom(gnd, label, average='macro')\n",
    "        dice += dice_coefficient_custom(gnd, label)\n",
    "\n",
    "    return [f1/batch_size, auc/batch_size, dice/batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of score \n",
    "f1_list = []; auc_list = []; dice_list = []\n",
    "print(f'Epochs\\t F1-score\\t ROC-AUC-score\\t Dice-coefficient')\n",
    "\n",
    "# move model to gpu\n",
    "model = model.to(device)\n",
    "# loop for original number of epochs\n",
    "for i in range(epochs):\n",
    "    # load the model states\n",
    "    model.load_state_dict(torch.load(f'../weights/T1/epoch_{i}.pth'))\n",
    "    # model in evaluation model -> batchnorm, dropout etc. adjusted accordingly\n",
    "    model.eval()\n",
    "    # evaluation score variables to store values over each epoch\n",
    "    f_one = roc_auc = dice_coef = 0 \n",
    "\n",
    "    for img, label in trainloader:\n",
    "        img, label = img.to(device), label.to(device) # to gpu\n",
    "        # deactivate autograd engine - reduce memory usage \n",
    "        with torch.no_grad(): \n",
    "            pred = model(img) # forward pass\n",
    "            # output of model is orderedDict\n",
    "#             pred = pred['out'] # Batchx21(class)xHxW\n",
    "            # evaluation\n",
    "            scores = evaluate_batch(label, pred)\n",
    "            # sum values\n",
    "            f_one += scores[0]\n",
    "            roc_auc += scores[1]\n",
    "            dice_coef += scores[2]\n",
    "    \n",
    "    print('{}\\t {:.3f}\\t\\t {:.3f}\\t\\t {:.3f}'.format(i , f_one/len(trainloader), roc_auc/len(trainloader), dice_coef/len(trainloader)))\n",
    "#     print(f'Epoch{i+1}:',f_one/len(trainloader), roc_auc/len(trainloader), dice_coef/len(trainloader))\n",
    "    # append to list (with averaged values over valid set)\n",
    "    f1_list.append(f_one/len(trainloader))\n",
    "    auc_list.append(roc_auc/len(trainloader))\n",
    "    dice_list.append(dice_coef/len(trainloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUf8J_s-gr21"
   },
   "source": [
    "### 1.8 Plot the evaluation metrics against epochs(1.0)\n",
    "In section 1.6 we saved the weights of the model after each epoch. In this section, you have to calculate the evaluation metrics after each epoch of training by loading the weights for each epoch. Once you have calculated the evaluation metrics for each epoch, plot them against the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fE2LNXlvgr21"
   },
   "outputs": [],
   "source": [
    "# PLOT\n",
    "X = range(1, epochs+1)\n",
    "plt.plot(X, f1_list, label=\"F1-score\")\n",
    "plt.plot(X, auc_list, label=\"AUC-ROC score\")\n",
    "plt.plot(X, dice_list, label=\"Dice coefficient\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Evaluation metrics score\")\n",
    "plt.title(\"Performance evalaution\")\n",
    "plt.legend() # add legend\n",
    "plt.savefig('results/T1_eval_metrics.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noSVRRkjgr22"
   },
   "source": [
    "### 1.9 Visualize results(0.5 points)\n",
    "For any 10 images in the dataset, show the images along the with their segmentation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NS50IL_c7Mf"
   },
   "outputs": [],
   "source": [
    "from datasets.custom_transforms import unNormalize, decode_segmap\n",
    "\n",
    "# move model to gpu\n",
    "model = model.to(device)\n",
    "# load the model states\n",
    "model.load_state_dict(torch.load(f'../weights/T1/epoch_{epochs-1}.pth'))\n",
    "# model.load_state_dict(torch.load(f'weights/T1/epoch_{13}.pth'))\n",
    "\n",
    "# model in evaluation model -> batchnorm, dropout etc. adjusted accordingly\n",
    "model.eval()\n",
    "# iterator on training data\n",
    "data = iter(trainloader)\n",
    "# init figure object\n",
    "fig = plt.figure(figsize=(10,40))\n",
    "pred_rgb = list()\n",
    "# for img, label in trainloader:\n",
    "for i in range(10):\n",
    "    imgs, labels = next(data) # next batch\n",
    "    # img, label = img.to(device).unsqueeze(0), label.to(device) # to gpu\n",
    "    # using just one image\n",
    "    img = imgs[0].to(device).unsqueeze(0) # to gpu\n",
    "    # gnd = np.asarray(label[0]) \n",
    "    # deactivate autograd engine - reduce memory usage \n",
    "    with torch.no_grad(): \n",
    "        pred = model(img) # forward pass\n",
    "        # output of model is orderedDict\n",
    "#         pred = pred['out'] # 21(class)xHxW\n",
    "        pred = pred.squeeze(0)\n",
    "        \n",
    "        # extract most probable class through C-dim \n",
    "        pred_label = torch.argmax(pred, dim=0).cpu().numpy()\n",
    "        # convert labels to color code\n",
    "#         pred_rgb = dst_train.decode_segmap(pred_label)\n",
    "        pred_rgb = decode_segmap(pred_label)\n",
    "\n",
    "        # plotting\n",
    "        # original image\n",
    "        fig.add_subplot(10, 3, 3*i+1)\n",
    "        img = imgs[0].data.cpu().numpy() # data in image and current form of matrix\n",
    "        img = unNormalize(img, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # unNormalize\n",
    "        img = img.transpose((1,2,0)).astype(np.uint8) # change dtype to correct format for display\n",
    "        plt.title('Original')\n",
    "        plt.imshow(img) # original\n",
    "        plt.axis('off')\n",
    "        # ground truth\n",
    "        fig.add_subplot(10, 3, 3*i+2)\n",
    "        label = labels[0].data.numpy() # data in image and current form of matrix\n",
    "        label = decode_segmap(label)\n",
    "#         label = dst_train.decode_segmap(label)\n",
    "        plt.title('Ground truth')\n",
    "        plt.imshow(label) \n",
    "        plt.axis('off')\n",
    "        # prediction\n",
    "        fig.add_subplot(10, 3, 3*i+3)\n",
    "        plt.title('Prediction')\n",
    "        plt.imshow(pred_rgb.astype(np.uint8))\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.savefig('results/T1_results.png', bbox_inches='tight')    \n",
    "plt.plot()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copy of segmentation_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f8ff8022dae4990b1ea432b2e9d3c07": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0858d6174ed4af7b49a103e61536b04",
      "max": 217800805,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d3610da2df64773b16b18f02fb4501a",
      "value": 217800805
     }
    },
    "13ea534c6ce64663ad8e4202a002f51d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1d3610da2df64773b16b18f02fb4501a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "58c5855440754107a3706f941d83e0f4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6dc48abab91f41e38a03efc97fc88f5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4e581c1ceb9408790f30b96cff73f99",
       "IPY_MODEL_ba09c1085fa74be4a05eced774156f7a"
      ],
      "layout": "IPY_MODEL_b69f7c47a904498986d106e1bf36d926"
     }
    },
    "8560f77e7d414d4ba09d847a451df0e0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92a04383007146e9b74ad6b15bc80413": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf9892dfa56c4cd6ae37fb8318044378",
      "placeholder": "​",
      "style": "IPY_MODEL_13ea534c6ce64663ad8e4202a002f51d",
      "value": " 208M/208M [00:02&lt;00:00, 92.4MB/s]"
     }
    },
    "b4e581c1ceb9408790f30b96cff73f99": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58c5855440754107a3706f941d83e0f4",
      "max": 178728960,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b5680358c1e74addbddab3e381c36019",
      "value": 178728960
     }
    },
    "b5680358c1e74addbddab3e381c36019": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b69f7c47a904498986d106e1bf36d926": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba09c1085fa74be4a05eced774156f7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d506f02006a648639ab6619118be8db1",
      "placeholder": "​",
      "style": "IPY_MODEL_e4d9fd672c704905a9a13c7d930dbad8",
      "value": " 170M/170M [00:04&lt;00:00, 44.4MB/s]"
     }
    },
    "bf9892dfa56c4cd6ae37fb8318044378": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d506f02006a648639ab6619118be8db1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4d9fd672c704905a9a13c7d930dbad8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e91dacd227004bfbb066cd835e22afd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0f8ff8022dae4990b1ea432b2e9d3c07",
       "IPY_MODEL_92a04383007146e9b74ad6b15bc80413"
      ],
      "layout": "IPY_MODEL_8560f77e7d414d4ba09d847a451df0e0"
     }
    },
    "f0858d6174ed4af7b49a103e61536b04": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
